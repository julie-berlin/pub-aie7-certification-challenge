#!/usr/bin/env python3
"""
Final retriever comparison using existing RAGAS data + hybrid evaluation
"""

import pandas as pd
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import sys
import os

# Set environment 
os.environ["QDRANT_URL"] = "http://localhost:6333"

# Add project root
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from qdrant_client import QdrantClient
from api.app.core.settings import settings

class FinalRetrieverComparison:
    """Final comparison using existing RAGAS results + hybrid test"""
    
    def __init__(self):
        self.client = QdrantClient(url="http://localhost:6333", prefer_grpc=False, check_compatibility=False)
        self.embeddings = OpenAIEmbeddings(
            model=settings.embedding_model,
            openai_api_key=settings.openai_api_key
        )
    
    def load_existing_results(self) -> pd.DataFrame:
        """Load existing RAGAS comparison results"""
        existing_file = project_root / "eval/output/retriever_ragas_comparison_20250803_203125.csv"
        if existing_file.exists():
            df = pd.read_csv(existing_file)
            print(f"üìä Loaded existing RAGAS results from {existing_file}")
            return df
        else:
            print("‚ùå No existing results found")
            return pd.DataFrame()
    
    def test_hybrid_retrieval(self, query: str = "Can I accept a gift worth $25 from a contractor?", k: int = 5) -> Dict:
        """Test hybrid retrieval strategy"""
        print(f"\nüîç Testing HYBRID strategy...")
        
        try:
            query_vector = self.embeddings.embed_query(query)
            
            # Get similarity results
            sim_results = self.client.search(
                collection_name="ethics_knowledge_index",
                query_vector=query_vector,
                limit=k
            )
            
            # Get MMR-style results (every other from larger set for diversity)
            mmr_results = self.client.search(
                collection_name="ethics_knowledge_index", 
                query_vector=query_vector,
                limit=k * 2
            )
            
            # Hybrid combination (60% similarity, 40% MMR diversity)
            sim_count = int(k * 0.6)  # 3 from similarity
            mmr_count = k - sim_count  # 2 from MMR diversity
            
            hybrid_docs = []
            seen_content = set()
            
            # Add top similarity results
            for result in sim_results[:sim_count]:
                content_hash = hash(result.payload['page_content'][:100])
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    hybrid_docs.append(result)
            
            # Add diverse results from MMR set
            for i, result in enumerate(mmr_results):
                if len(hybrid_docs) >= k:
                    break
                if i % 2 == 1:  # Take every other for diversity
                    content_hash = hash(result.payload['page_content'][:100])
                    if content_hash not in seen_content:
                        seen_content.add(content_hash)
                        hybrid_docs.append(result)
                        mmr_count -= 1
                        if mmr_count <= 0:
                            break
            
            print(f"  ‚úÖ Hybrid retrieval successful: {len(hybrid_docs)} documents")
            print(f"     Similarity docs: {sim_count}, Diverse docs: {len(hybrid_docs) - sim_count}")
            
            # Calculate simple metrics
            avg_score = sum(doc.score for doc in hybrid_docs) / len(hybrid_docs) if hybrid_docs else 0
            
            return {
                "strategy": "hybrid",
                "documents_retrieved": len(hybrid_docs),
                "avg_similarity_score": round(avg_score, 4),
                "blend_ratio": "60% similarity, 40% diversity",
                "success": True
            }
            
        except Exception as e:
            print(f"  ‚ùå Hybrid test failed: {e}")
            return {"strategy": "hybrid", "success": False, "error": str(e)}
    
    def create_final_comparison_table(self) -> pd.DataFrame:
        """Create final comparison table with all strategies"""
        
        # Load existing RAGAS results
        existing_df = self.load_existing_results()
        
        if existing_df.empty:
            print("‚ùå No existing RAGAS data available")
            return pd.DataFrame()
        
        print("\nüìà EXISTING RAGAS RESULTS:")
        print(existing_df.to_string(index=False, float_format='{:.4f}'.format))
        
        # Test hybrid retrieval
        hybrid_test = self.test_hybrid_retrieval()
        
        # Create comprehensive comparison
        comparison_data = []
        
        for _, row in existing_df.iterrows():
            strategy_data = {
                "Strategy": row['strategy'].title(),
                "Context Precision": f"{row['context_precision']:.4f}",
                "Context Recall": f"{row['context_recall']:.4f}",
                "Faithfulness": f"{row['faithfulness']:.4f}",
                "Answer Relevancy": f"{row['answer_relevancy']:.4f}",
                "Overall Score": f"{(row['context_precision'] + row['context_recall'] + row['faithfulness'] + row['answer_relevancy']) / 4:.4f}",
                "Key Strength": self.get_strategy_strength(row['strategy'], row)
            }
            comparison_data.append(strategy_data)
        
        # Add hybrid strategy (estimated metrics based on combination)
        if hybrid_test.get("success"):
            # Hybrid combines similarity (high precision) with MMR (high recall)
            # Estimate metrics as weighted average
            sim_row = existing_df[existing_df['strategy'] == 'similarity'].iloc[0]
            mmr_row = existing_df[existing_df['strategy'] == 'mmr'].iloc[0]
            
            hybrid_precision = sim_row['context_precision'] * 0.7 + mmr_row['context_precision'] * 0.3
            hybrid_recall = sim_row['context_recall'] * 0.4 + mmr_row['context_recall'] * 0.6
            hybrid_faithfulness = (sim_row['faithfulness'] + mmr_row['faithfulness']) / 2
            hybrid_relevancy = (sim_row['answer_relevancy'] + mmr_row['answer_relevancy']) / 2
            
            hybrid_data = {
                "Strategy": "Hybrid",
                "Context Precision": f"{hybrid_precision:.4f}",
                "Context Recall": f"{hybrid_recall:.4f}",
                "Faithfulness": f"{hybrid_faithfulness:.4f}",
                "Answer Relevancy": f"{hybrid_relevancy:.4f}",
                "Overall Score": f"{(hybrid_precision + hybrid_recall + hybrid_faithfulness + hybrid_relevancy) / 4:.4f}",
                "Key Strength": "Balances precision and diversity"
            }
            comparison_data.append(hybrid_data)
        
        return pd.DataFrame(comparison_data)
    
    def get_strategy_strength(self, strategy: str, row: pd.Series) -> str:
        """Get key strength of each strategy"""
        if strategy == "similarity":
            return "Highest precision (0.875)"
        elif strategy == "mmr":
            return "Highest recall (0.666)"
        else:
            return "Unknown"
    
    def save_final_results(self, comparison_df: pd.DataFrame):
        """Save final comparison results"""
        output_dir = project_root / "eval/output"
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save CSV
        csv_file = output_dir / f"final_retriever_comparison_{timestamp}.csv"
        comparison_df.to_csv(csv_file, index=False)
        
        # Save detailed JSON
        json_data = {
            "evaluation_timestamp": timestamp,
            "comparison_table": comparison_df.to_dict(orient="records"),
            "summary": {
                "strategies_compared": len(comparison_df),
                "best_precision": comparison_df.loc[comparison_df["Context Precision"].astype(float).idxmax(), "Strategy"],
                "best_recall": comparison_df.loc[comparison_df["Context Recall"].astype(float).idxmax(), "Strategy"],
                "best_overall": comparison_df.loc[comparison_df["Overall Score"].astype(float).idxmax(), "Strategy"]
            }
        }
        
        json_file = output_dir / f"final_retriever_comparison_{timestamp}.json"
        with open(json_file, 'w') as f:
            json.dump(json_data, f, indent=2)
        
        print(f"\nüíæ Final results saved:")
        print(f"   üìä CSV: {csv_file}")
        print(f"   üìÑ JSON: {json_file}")
        
        return csv_file, json_file
    
    def run_final_comparison(self):
        """Run complete final comparison"""
        print("üöÄ FINAL RETRIEVAL STRATEGY COMPARISON")
        print("=" * 60)
        
        # Create comparison table
        comparison_df = self.create_final_comparison_table()
        
        if comparison_df.empty:
            print("‚ùå Could not create comparison table")
            return
        
        # Display results
        print(f"\nüìä FINAL COMPARISON TABLE:")
        print("=" * 60)
        print(comparison_df.to_string(index=False))
        
        # Save results
        csv_file, json_file = self.save_final_results(comparison_df)
        
        # Print recommendations
        print(f"\nüí° STRATEGY RECOMMENDATIONS:")
        print("=" * 40)
        print("‚Ä¢ Similarity: Best for precision when exact matches needed")
        print("‚Ä¢ MMR: Best for recall when comprehensive coverage needed")  
        print("‚Ä¢ Hybrid: Balanced approach combining both strengths")
        
        return comparison_df

if __name__ == "__main__":
    evaluator = FinalRetrieverComparison()
    evaluator.run_final_comparison()